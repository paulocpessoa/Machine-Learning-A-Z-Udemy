{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "C:\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "C:\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential  # Usamos para inicializar nossa rede neural sequencialmente camada a camada.\n",
    "from keras.layers import Convolution2D  # Usado no primeiro passado para fazer as camadas convolucionais.\n",
    "from keras.layers import MaxPooling2D  # Usado para fazer o pooling.\n",
    "from keras.layers import Flatten  # Usado para transformar o pooling em um vetor usado como variaveis de entrada.\n",
    "from keras.layers import Dense  # Usado para conectar as camadas formando uma rede neural.\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# step - 1 - Convolution\n",
    "\n",
    "classifier.add(Convolution2D(32, 3, 3,  # 32 features detector de 3x3.\n",
    "# Vamos reduzir as imagens para 64x64 e usar 3 matrizes de numeros por imagem,\n",
    "# uma pra cada cor. Está nessa ordem porque estamos usando tensorflow backend.\n",
    "                             input_shape = (64, 64, 3),\n",
    "# Vamos usar o Relu como activation function para tirar a linearidade.\n",
    "                             activation = 'relu'))\n",
    "\n",
    "# step -2 -- Pooling\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "# Se quiser adicionar uma outra camada, pode criar uma  outra com 64\n",
    "classifier.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# step - 3 -- Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step -4 Full Connection\n",
    "# Foi um número testado, não foi uma conta feita pelo professor.\n",
    "classifier.add(Dense(output_dim=128, activation='relu'))\n",
    "# Queremos classificar entre cachorro e gato, por isso uma variável de saída.\n",
    "classifier.add(Dense(output_dim=1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(\n",
    "\toptimizer='adam',  # É um stochastic gradient descend usado para descobrir os pesos.\n",
    "\tloss='binary_crossentropy',  # Para calcular o cost function, essa função é comum em sigmoids.\n",
    "\tmetrics=['accuracy']  # A metrica para verificar a performance do modelo.\n",
    ")\n",
    "\n",
    "# part - 2 -- Fitting the CNN to the images\n",
    "# Apesar de termos milhares de imagem, ainda não é muito, nesse topico vamos fazer algumas alterações nas imagens,\n",
    "# de forma que teremos mais imagens para aprender. Essas alterações podem ser rotações na imagem entre outras coisas.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#region Esse código foi pego de https://keras.io/preprocessing/image/\n",
    "train_datagen = ImageDataGenerator(\n",
    "\trescale=1. / 255,  # Reescala os pixels entre 1 e 255.\n",
    "\tshear_range=0.2,  # A rotação que pode ser aplicada em graus sentindo anti-horário.\n",
    "\tzoom_range=0.2,  # O zoom que pode ser aplicado nas imagens.\n",
    "\thorizontal_flip=True  # Diz que nossas imagens serão giradas horizontalmente.\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)  #\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "\t'C:/git/Machine-Learning-A-Z-Udemy/part_8-deep-learning/convolutional_neural_network/dataset/training_set',  # path/to/data/\n",
    "\ttarget_size=(64, 64),\n",
    "\tbatch_size=32,\n",
    "\tclass_mode='binary'  # Aqui indicamos que a saída é gato/cachorro, portanto, binária.\n",
    ")\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "\t'C:/git/Machine-Learning-A-Z-Udemy/part_8-deep-learning/convolutional_neural_network/dataset/test_set',\n",
    "\ttarget_size=(64, 64),\n",
    "\tbatch_size=32,\n",
    "\tclass_mode='binary'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "C:\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=250, epochs=25, validation_steps=2000)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.6823 - accuracy: 0.5700 - val_loss: 0.7248 - val_accuracy: 0.5934\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.6380 - accuracy: 0.6388 - val_loss: 0.7616 - val_accuracy: 0.6426\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 87s 349ms/step - loss: 0.6009 - accuracy: 0.6785 - val_loss: 0.7549 - val_accuracy: 0.6993\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 90s 361ms/step - loss: 0.5607 - accuracy: 0.7049 - val_loss: 0.6207 - val_accuracy: 0.7338\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 93s 371ms/step - loss: 0.5451 - accuracy: 0.7222 - val_loss: 0.5394 - val_accuracy: 0.7403\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.5181 - accuracy: 0.7452 - val_loss: 0.4785 - val_accuracy: 0.7517\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.4895 - accuracy: 0.7645 - val_loss: 0.5169 - val_accuracy: 0.7533\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.4733 - accuracy: 0.7681 - val_loss: 0.4714 - val_accuracy: 0.7445\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 86s 344ms/step - loss: 0.4485 - accuracy: 0.7862 - val_loss: 0.4497 - val_accuracy: 0.7489\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 89s 354ms/step - loss: 0.4351 - accuracy: 0.7949 - val_loss: 0.4392 - val_accuracy: 0.7544\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 93s 371ms/step - loss: 0.4177 - accuracy: 0.8098 - val_loss: 0.3665 - val_accuracy: 0.7650\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 92s 370ms/step - loss: 0.4011 - accuracy: 0.8183 - val_loss: 0.3582 - val_accuracy: 0.7715\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.3822 - accuracy: 0.8230 - val_loss: 0.4892 - val_accuracy: 0.7811\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 0.3589 - accuracy: 0.8418 - val_loss: 0.6018 - val_accuracy: 0.7677\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 109s 434ms/step - loss: 0.3458 - accuracy: 0.8469 - val_loss: 0.4999 - val_accuracy: 0.7792\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 103s 414ms/step - loss: 0.3195 - accuracy: 0.8645 - val_loss: 0.4477 - val_accuracy: 0.7921\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 95s 380ms/step - loss: 0.2999 - accuracy: 0.8711 - val_loss: 0.4616 - val_accuracy: 0.7731\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 98s 392ms/step - loss: 0.2817 - accuracy: 0.8810 - val_loss: 0.5375 - val_accuracy: 0.7884\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2751 - accuracy: 0.8850 - val_loss: 0.9176 - val_accuracy: 0.7741\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 99s 395ms/step - loss: 0.2550 - accuracy: 0.8951 - val_loss: 0.1777 - val_accuracy: 0.7861\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 104s 416ms/step - loss: 0.2517 - accuracy: 0.8970 - val_loss: 0.9712 - val_accuracy: 0.7881\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 98s 394ms/step - loss: 0.2230 - accuracy: 0.9078 - val_loss: 0.5042 - val_accuracy: 0.7841\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 95s 380ms/step - loss: 0.2132 - accuracy: 0.9172 - val_loss: 0.5039 - val_accuracy: 0.7787\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 94s 375ms/step - loss: 0.1997 - accuracy: 0.9190 - val_loss: 0.3977 - val_accuracy: 0.7866\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 96s 382ms/step - loss: 0.1874 - accuracy: 0.9266 - val_loss: 0.5245 - val_accuracy: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e9b5527908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "    training_set,\n",
    "    samples_per_epoch=8000,\n",
    "    nb_epoch=25,\n",
    "    validation_data=test_set,\n",
    "    nb_val_samples=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-cpu",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
